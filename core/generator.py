"""
=============================================================================
ğŸ° Ù…ÙˆÙ„Ø¯ Ø§Ù„ØªØ°Ø§ÙƒØ± Ø§Ù„Ø°ÙƒÙŠ Ø§Ù„Ù…Ø­Ø³Ù‘Ù† Ù…Ø¹ ØªØ­Ø³ÙŠÙ†Ø§Øª Ø§Ù„Ø£Ø¯Ø§Ø¡
=============================================================================
"""

import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Optional, Set
from collections import Counter
import random
import itertools
from concurrent.futures import ThreadPoolExecutor, as_completed

from config.settings import Config
from utils.logger import logger
from utils.performance import PerformanceBenchmark
from core.analyzer import AdvancedAnalyzer


class SmartGenerator:
    """Ù…ÙˆÙ„Ø¯ ØªØ°Ø§ÙƒØ± Ø°ÙƒÙŠ Ù…Ø¹ ÙÙ„Ø§ØªØ± Ù…ØªÙ‚Ø¯Ù…Ø© ÙˆØªØ­Ø³ÙŠÙ†Ø§Øª Ø£Ø¯Ø§Ø¡"""
    
    def __init__(self, analyzer: AdvancedAnalyzer):
        self.analyzer = analyzer
        self.benchmark = PerformanceBenchmark()
        self.cache: Dict[str, List[List[int]]] = {}
    
    def generate_tickets(
        self,
        count: int,
        size: int = 6,
        constraints: Optional[Dict] = None,
        use_cache: bool = True
    ) -> List[List[int]]:
        """ØªÙˆÙ„ÙŠØ¯ ØªØ°Ø§ÙƒØ± Ù…Ø¹ ÙÙ„Ø§ØªØ± Ù…Ø­Ø³Ù†Ø© - ÙŠØ¨Ø­Ø« ÙÙŠ ÙƒÙ„ Ø§Ù„ØªØ°Ø§ÙƒØ± Ø§Ù„Ù…Ù…ÙƒÙ†Ø© Ø­ØªÙ‰ ÙŠØ¬Ø¯ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨"""
        
        if constraints is None:
            constraints = {}
        
        cache_key = self._generate_cache_key(count, size, constraints)
        if use_cache and cache_key in self.cache:
            logger.logger.info(f"ğŸ¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Cache Ù„Ù„ØªÙˆÙ„ÙŠØ¯ - Ù…ÙØªØ§Ø­: {cache_key[:50]}...")
            return self.cache[cache_key].copy()
        
        op_id = logger.start_operation('ticket_generation', {
            'count': count,
            'size': size,
            'constraints': constraints
        })
        
        try:
            with self.benchmark.monitor_operation('generation'):
                pool = self._prepare_number_pool(constraints)
                
                if len(pool) < size:
                    error_msg = f"âŒ Ø¹Ø¯Ø¯ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…ØªØ§Ø­Ø© ({len(pool)}) Ø£Ù‚Ù„ Ù…Ù† Ø­Ø¬Ù… Ø§Ù„ØªØ°ÙƒØ±Ø© ({size})"
                    logger.logger.error(error_msg)
                    raise ValueError(error_msg)
                
                # âœ… Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø´Ø§Ù…Ù„: ÙŠØ¨Ø­Ø« ÙÙŠ ÙƒÙ„ Ø§Ù„ØªØ°Ø§ÙƒØ± Ø§Ù„Ù…Ù…ÙƒÙ†Ø© Ø­ØªÙ‰ ÙŠØ¬Ø¯ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨
                tickets = self._exhaustive_search(pool, size, count, constraints)
                
                if use_cache and len(tickets) > 0:
                    self.cache[cache_key] = tickets.copy()
                    self._clean_cache()
                
                logger.end_operation(op_id, 'completed', {
                    'generated_count': len(tickets),
                    'success_rate': round(len(tickets) / count * 100, 2) if count > 0 else 0,
                    'cache_used': use_cache,
                    'cache_key': cache_key[:30]
                })
                
                return tickets
                
        except Exception as e:
            logger.end_operation(op_id, 'failed', {'error': str(e)})
            raise
    
    def _exhaustive_search(self, pool: List[int], size: int, 
                           count: int, constraints: Dict) -> List[List[int]]:
        """
        Ø¨Ø­Ø« Ø´Ø§Ù…Ù„: ÙŠØ¨Ø­Ø« ÙÙŠ ÙƒÙ„ Ø§Ù„ØªÙˆÙ„ÙŠÙØ§Øª Ø§Ù„Ù…Ù…ÙƒÙ†Ø© Ø­ØªÙ‰ ÙŠØ¬Ø¯ Ø§Ù„Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨.
        ÙŠØ³ØªØ®Ø¯Ù… Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© Ø°ÙƒÙŠØ©: Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø£ÙˆÙ„Ø§Ù‹ØŒ Ø«Ù… Ø§Ø³ØªÙ†Ø²Ø§Ù Ø´Ø§Ù…Ù„ Ø¥Ø°Ø§ Ù„Ø²Ù….
        """
        import math
        
        total_combinations = math.comb(len(pool), size)
        tickets_set: Set[tuple] = set()
        
        # âœ… Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: ØªÙˆÙ„ÙŠØ¯ Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø³Ø±ÙŠØ¹ (Ù„Ù„Ø´Ø±ÙˆØ· ØºÙŠØ± Ø§Ù„ØµØ§Ø±Ù…Ø© Ø¬Ø¯Ø§Ù‹)
        max_random_attempts = max(count * 5000, 50000)
        attempts = 0
        
        while len(tickets_set) < count and attempts < max_random_attempts:
            attempts += 1
            ticket = tuple(sorted(random.sample(pool, size)))
            
            if (self._satisfies_basic_constraints(list(ticket), constraints) and
                    self._satisfies_advanced_constraints(list(ticket), constraints)):
                tickets_set.add(ticket)
        
        # âœ… Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯ Ø§Ù„ÙƒÙ…ÙŠØ© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©ØŒ Ù†Ø¨Ø­Ø« Ø§Ø³ØªÙ†Ø²Ø§ÙÙŠØ§Ù‹ ÙÙŠ ÙƒÙ„ Ø§Ù„ØªÙˆÙ„ÙŠÙØ§Øª
        if len(tickets_set) < count and total_combinations <= 2_000_000:
            logger.logger.info(f"ğŸ” Ø¨Ø¯Ø¡ Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø´Ø§Ù…Ù„ ÙÙŠ {total_combinations:,} ØªÙˆÙ„ÙŠÙØ© Ù…Ù…ÙƒÙ†Ø©...")
            
            # Ø®Ù„Ø· Ø§Ù„Ù€ pool Ù„ØªÙ†ÙˆÙŠØ¹ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
            shuffled_pool = pool.copy()
            random.shuffle(shuffled_pool)
            
            for combo in itertools.combinations(shuffled_pool, size):
                ticket = tuple(sorted(combo))
                if ticket in tickets_set:
                    continue
                    
                if (self._satisfies_basic_constraints(list(ticket), constraints) and
                        self._satisfies_advanced_constraints(list(ticket), constraints)):
                    tickets_set.add(ticket)
                    
                if len(tickets_set) >= count:
                    break
        
        elif len(tickets_set) < count and total_combinations > 2_000_000:
            # âœ… Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3: Ù„Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„ÙƒØ«ÙŠØ±Ø©ØŒ Ù†Ø²ÙŠØ¯ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ±
            logger.logger.info(f"ğŸ” Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ù…ÙˆØ³Ø¹... ({total_combinations:,} ØªÙˆÙ„ÙŠÙØ© Ù…Ù…ÙƒÙ†Ø©)")
            extra_attempts = 0
            max_extra = count * 100_000
            
            while len(tickets_set) < count and extra_attempts < max_extra:
                extra_attempts += 1
                ticket = tuple(sorted(random.sample(pool, size)))
                
                if (self._satisfies_basic_constraints(list(ticket), constraints) and
                        self._satisfies_advanced_constraints(list(ticket), constraints)):
                    tickets_set.add(ticket)
        
        return [list(t) for t in list(tickets_set)[:count]]
    
    def _prepare_number_pool(self, constraints: Dict) -> List[int]:
        """ØªØ­Ø¶ÙŠØ± Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ù…Ø¹ ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø§Ø³ØªØ¨Ø¹Ø§Ø¯"""
        pool = list(range(Config.MIN_NUMBER, Config.MAX_NUMBER + 1))
        
        if 'exclude' in constraints:
            exclude_set = set(constraints['exclude'])
            pool = [n for n in pool if n not in exclude_set]
        
        if constraints.get('filter_low_freq', False):
            freq_values = list(self.analyzer.freq.values())
            if freq_values:
                avg_freq = np.mean(freq_values)
                pool = [n for n in pool if self.analyzer.freq.get(n, 0) >= avg_freq * 0.5]
        
        return pool
    
    def _generate_small_batch(self, pool: List[int], size: int, 
                            count: int, constraints: Dict) -> List[List[int]]:
        """ØªÙˆÙ„ÙŠØ¯ Ø¯ÙØ¹Ø§Øª ØµØºÙŠØ±Ø© (<= 10)"""
        return self._exhaustive_search(pool, size, count, constraints)
    
    def _generate_medium_batch(self, pool: List[int], size: int, 
                             count: int, constraints: Dict) -> List[List[int]]:
        """ØªÙˆÙ„ÙŠØ¯ Ø¯ÙØ¹Ø§Øª Ù…ØªÙˆØ³Ø·Ø© (<= 100)"""
        return self._exhaustive_search(pool, size, count, constraints)
    
    def _generate_large_batch(self, pool: List[int], size: int, 
                            count: int, constraints: Dict) -> List[List[int]]:
        """ØªÙˆÙ„ÙŠØ¯ Ø¯ÙØ¹Ø§Øª ÙƒØ¨ÙŠØ±Ø© (> 100)"""
        return self._exhaustive_search(pool, size, count, constraints)
    
    def _generate_batch_parallel(self, pool: List[int], size: int, 
                               batch_size: int, constraints: Dict) -> List[List[int]]:
        """ØªÙˆÙ„ÙŠØ¯ Ø¯ÙØ¹Ø© Ø¨Ø§Ù„ØªÙˆØ§Ø²ÙŠ"""
        batch_tickets = []
        
        for _ in range(batch_size):
            if len(pool) < size:
                break
            ticket = sorted(random.sample(pool, size))
            
            if (self._satisfies_basic_constraints(ticket, constraints) and 
                self._satisfies_advanced_constraints(ticket, constraints)):
                batch_tickets.append(ticket)
        
        return batch_tickets
    
    def _filter_batch_vectorized(self, batch: np.ndarray, constraints: Dict) -> np.ndarray:
        """ØªØµÙÙŠØ© Ø§Ù„Ø¯ÙØ¹Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… vectorization"""
        if batch.size == 0:
            return batch
        
        mask = np.ones(len(batch), dtype=bool)
        
        if 'sum_range' in constraints:
            min_sum, max_sum = constraints['sum_range']
            row_sums = batch.sum(axis=1)
            mask &= (row_sums >= min_sum) & (row_sums <= max_sum)
        
        if 'odd' in constraints:
            target_odd = constraints['odd']
            odd_counts = (batch % 2).sum(axis=1)
            mask &= (odd_counts == target_odd)
        
        return batch[mask]
    
    def _satisfies_basic_constraints(self, ticket: List[int], constraints: Dict) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‚ÙŠÙˆØ¯ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©"""
        if 'odd' in constraints:
            odd_count = sum(1 for n in ticket if n % 2)
            if odd_count != constraints['odd']:
                return False
        
        if 'sum_range' in constraints:
            min_sum, max_sum = constraints['sum_range']
            ticket_sum = sum(ticket)
            if not (min_sum <= ticket_sum <= max_sum):
                return False
        
        if 'fixed' in constraints:
            fixed_set = set(constraints['fixed'])
            if not fixed_set.issubset(set(ticket)):
                return False
        
        return True
    
    def _satisfies_advanced_constraints(self, ticket: List[int], constraints: Dict) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ù‚ÙŠÙˆØ¯ Ø§Ù„Ù…ØªÙ‚Ø¯Ù…Ø©"""
        if 'consecutive' in constraints:
            consec_count = sum(1 for i in range(len(ticket)-1) 
                             if ticket[i+1] - ticket[i] == 1)
            if consec_count != constraints['consecutive']:
                return False
        
        if 'shadows' in constraints:
            shadows_count = sum(1 for c in Counter([n % 10 for n in ticket]).values() 
                              if c > 1)
            if shadows_count != constraints['shadows']:
                return False
        
        if 'hot_min' in constraints:
            hot_count = len(set(ticket) & self.analyzer.hot)
            if hot_count < constraints['hot_min']:
                return False
        
        if 'cold_max' in constraints:
            cold_count = len(set(ticket) & self.analyzer.cold)
            if cold_count > constraints['cold_max']:
                return False
        
        if 'last_match' in constraints:
            match_count = len(set(ticket) & self.analyzer.last_draw)
            if match_count != constraints['last_match']:
                return False
        
        return True
    
    def _apply_advanced_filters(self, tickets: List[List[int]], constraints: Dict) -> List[List[int]]:
        """ØªØ·Ø¨ÙŠÙ‚ ÙÙ„Ø§ØªØ± Ù…ØªÙ‚Ø¯Ù…Ø© Ø¨Ø¹Ø¯ Ø§Ù„ØªÙˆÙ„ÙŠØ¯"""
        return [
            ticket for ticket in tickets
            if self._satisfies_advanced_constraints(ticket, constraints)
        ]
    
    def generate_markov_based(self, count: int, size: int = 6) -> List[List[int]]:
        """ØªÙˆÙ„ÙŠØ¯ ØªØ°Ø§ÙƒØ± Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Markov"""
        op_id = logger.start_operation('markov_generation', {
            'count': count,
            'size': size
        })
        
        try:
            with self.benchmark.monitor_operation('markov_generation'):
                tickets = []
                last_nums = sorted(list(self.analyzer.last_draw))
                
                for _ in range(count):
                    predictions = self.analyzer.get_markov_prediction(last_nums, top_n=15)
                    
                    if not predictions:
                        ticket = sorted(random.sample(range(Config.MIN_NUMBER, Config.MAX_NUMBER + 1), size))
                    else:
                        # âœ… Ø¥ØµÙ„Ø§Ø­: ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ Ù‚ÙˆØ§Ø¦Ù… ÙÙˆØ±Ø§Ù‹ ÙˆØªÙˆØ³ÙŠØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­
                        cand_nums = [num for num, _ in predictions]
                        cand_weights = [w for _, w in predictions]
                        
                        # ØªÙƒÙ…Ù„Ø© Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† ÙƒØ§ÙÙŠØ©
                        if len(cand_nums) < size:
                            all_nums = set(range(Config.MIN_NUMBER, Config.MAX_NUMBER + 1))
                            remaining = list(all_nums - set(cand_nums))
                            random.shuffle(remaining)
                            needed = size * 2 - len(cand_nums)
                            extra_nums = remaining[:needed]
                            # Ù…Ù†Ø­ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ© ÙˆØ²Ù†Ø§Ù‹ Ù…Ù†Ø®ÙØ¶Ø§Ù‹
                            avg_weight = float(np.mean(cand_weights)) if cand_weights else 0.1
                            extra_weights = [avg_weight * 0.1] * len(extra_nums)
                            cand_nums = cand_nums + extra_nums
                            cand_weights = cand_weights + extra_weights
                        
                        # ØªØ·Ø¨ÙŠØ¹ Ø§Ù„Ø£ÙˆØ²Ø§Ù†
                        total_w = sum(cand_weights)
                        if total_w > 0:
                            cand_weights = [w / total_w for w in cand_weights]
                        else:
                            cand_weights = [1.0 / len(cand_nums)] * len(cand_nums)
                        
                        # âœ… Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø¨Ù†ÙØ³ Ø·ÙˆÙ„ Ø§Ù„Ù…Ø±Ø´Ø­ÙŠÙ† ØªÙ…Ø§Ù…Ø§Ù‹
                        n = len(cand_nums)
                        weights_arr = np.array(cand_weights[:n])
                        weights_arr = weights_arr / weights_arr.sum()  # Ø¥Ø¹Ø§Ø¯Ø© ØªØ·Ø¨ÙŠØ¹
                        
                        selected = np.random.choice(
                            cand_nums[:n],
                            size=min(size, n),
                            replace=False,
                            p=weights_arr
                        )
                        ticket = sorted(selected.tolist())
                        
                        # Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø­Ø¬Ù… Ø£Ù‚Ù„ Ù…Ù† Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ØŒ Ù†ÙƒÙ…Ù„ Ø¹Ø´ÙˆØ§Ø¦ÙŠØ§Ù‹
                        if len(ticket) < size:
                            remaining = list(set(range(Config.MIN_NUMBER, Config.MAX_NUMBER + 1)) - set(ticket))
                            ticket.extend(random.sample(remaining, size - len(ticket)))
                            ticket = sorted(ticket)
                    
                    if ticket not in tickets:
                        tickets.append(ticket)
                
                logger.end_operation(op_id, 'completed', {
                    'generated_count': len(tickets),
                    'markov_used': len(predictions) > 0 if 'predictions' in dir() else False
                })
                
                return tickets
                
        except Exception as e:
            logger.end_operation(op_id, 'failed', {'error': str(e)})
            raise
    
    def _generate_cache_key(self, count: int, size: int, constraints: Dict) -> str:
        """ØªÙˆÙ„ÙŠØ¯ Ù…ÙØªØ§Ø­ Cache ÙØ±ÙŠØ¯"""
        import hashlib
        import json
        
        # âœ… Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„ØªØ³Ù„Ø³Ù„ Ø¥Ù„Ù‰ JSON
        safe_constraints = {}
        for k, v in constraints.items():
            if isinstance(v, set):
                safe_constraints[k] = sorted(list(v))
            elif isinstance(v, (list, tuple)):
                safe_constraints[k] = list(v)
            else:
                safe_constraints[k] = v
        
        data = {
            'count': count,
            'size': size,
            'constraints': safe_constraints,
            'analyzer_hash': hash(str(sorted(self.analyzer.freq.items())))
        }
        
        data_str = json.dumps(data, sort_keys=True)
        return hashlib.md5(data_str.encode()).hexdigest()
    
    def _clean_cache(self):
        """ØªÙ†Ø¸ÙŠÙ Cache Ø§Ù„Ù‚Ø¯ÙŠÙ…"""
        max_cache_size = 100
        
        if len(self.cache) > max_cache_size:
            keys_to_remove = list(self.cache.keys())[:len(self.cache) - max_cache_size]
            for key in keys_to_remove:
                del self.cache[key]
    
    def generate_with_ml(self, count: int, size: int = 6, 
                        model_name: str = 'random_forest') -> List[List[int]]:
        """ØªÙˆÙ„ÙŠØ¯ ØªØ°Ø§ÙƒØ± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ†Ø¨Ø¤Ø§Øª ML"""
        op_id = logger.start_operation('ml_generation', {
            'count': count,
            'size': size,
            'model': model_name
        })
        
        try:
            with self.benchmark.monitor_operation('ml_generation'):
                tickets = []
                
                for _ in range(count):
                    ticket = self._generate_ml_inspired_ticket(size)
                    if ticket not in tickets:
                        tickets.append(ticket)
                
                logger.end_operation(op_id, 'completed', {
                    'generated_count': len(tickets),
                    'model_used': model_name
                })
                
                return tickets
                
        except Exception as e:
            logger.end_operation(op_id, 'failed', {'error': str(e)})
            raise
    
    def _generate_ml_inspired_ticket(self, size: int) -> List[int]:
        """ØªÙˆÙ„ÙŠØ¯ ØªØ°ÙƒØ±Ø© Ù…Ø³ØªÙˆØ­Ø§Ø© Ù…Ù† ØªÙ†Ø¨Ø¤Ø§Øª ML"""
        pool = list(range(Config.MIN_NUMBER, Config.MAX_NUMBER + 1))
        
        weights = np.ones(len(pool))
        for i, num in enumerate(pool):
            if num in self.analyzer.hot:
                weights[i] = 2.0
            elif num in self.analyzer.cold:
                weights[i] = 0.5
        
        weights = weights / weights.sum()
        
        ticket = np.random.choice(
            pool,
            size=size,
            replace=False,
            p=weights
        )
        
        return sorted(ticket.tolist())
    
    def get_generation_stats(self) -> Dict:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„ØªÙˆÙ„ÙŠØ¯"""
        return {
            'cache_size': len(self.cache),
            'performance_stats': self.benchmark.get_performance_report('generation'),
            'generator_info': {
                'class': self.__class__.__name__,
                'analyzer_initialized': self.analyzer is not None,
                'methods_available': [
                    'generate_tickets',
                    'generate_markov_based',
                    'generate_with_ml'
                ]
            }
        }

"""
=============================================================================
ğŸ§  Ù†Ø¸Ø§Ù… Machine Learning Ù…ØªÙ‚Ø¯Ù… Ù„Ù„ØªÙ†Ø¨Ø¤
=============================================================================
"""

import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Optional, Set
from collections import Counter
from itertools import chain   # âœ… Ø¥Ø¶Ø§ÙØ© Ø§Ø³ØªÙŠØ±Ø§Ø¯ chain Ø§Ù„Ù…ÙÙ‚ÙˆØ¯
import joblib
import os
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import accuracy_score, precision_score, recall_score
import warnings
warnings.filterwarnings('ignore')

from config.settings import Config
from utils.logger import logger
from utils.performance import PerformanceBenchmark


class LotteryPredictor:
    """Ù†Ø¸Ø§Ù… ØªÙ†Ø¨Ø¤ Ù…ØªÙ‚Ø¯Ù… Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Machine Learning"""
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.feature_importance = {}
        self.is_trained = False
        self.benchmark = PerformanceBenchmark()
        
        self._initialize_models()
    
    def _initialize_models(self):
        """ØªÙ‡ÙŠØ¦Ø© Ù†Ù…Ø§Ø°Ø¬ ML Ù…Ø®ØªÙ„ÙØ©"""
        self.models['random_forest'] = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        
        self.models['gradient_boosting'] = GradientBoostingClassifier(
            n_estimators=100,
            learning_rate=0.1,
            max_depth=5,
            random_state=42
        )
        
        for model_name in self.models:
            self.scalers[model_name] = StandardScaler()
    
    def prepare_features(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """Ø¥Ø¹Ø¯Ø§Ø¯ features Ù…ØªÙ‚Ø¯Ù…Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª"""
        operation_id = logger.start_operation('feature_preparation', {
            'total_draws': len(df),
            'models_count': len(self.models)
        })
        
        features_list = []
        labels_list = []
        
        try:
            for i in range(len(df) - 2):
                current = df.iloc[i]['numbers']
                next_draw = df.iloc[i + 1]['numbers']
                future_draw = df.iloc[i + 2]['numbers']
                
                # âœ… chain Ù…Ø³ØªÙˆØ±Ø¯Ø© Ø§Ù„Ø¢Ù† Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­
                freq_counter = Counter(list(chain.from_iterable(df.iloc[:i+1]['numbers'])))
                
                basic_features = [
                    *sorted(current),
                    sum(current),
                    sum(1 for n in current if n % 2),
                    sum(1 for j in range(len(current)-1) if current[j+1] - current[j] == 1),
                    current[-1] - current[0] if current else 0,
                    float(np.mean(current)),
                    float(np.std(current))
                ]
                
                statistical_features = [
                    float(np.mean([freq_counter.get(n, 0) for n in current])),
                    float(np.std([freq_counter.get(n, 0) for n in current])),
                    len(set(current) & set(next_draw)),
                ]
                
                pattern_features = [
                    len(set([n % 10 for n in current])),
                    sum(1 for n in current if self._is_prime(n)),
                    self._calculate_balance(current)
                ]
                
                feature_vector = basic_features + statistical_features + pattern_features
                features_list.append(feature_vector)
                
                # Label: Ù‡Ù„ ÙŠØ¸Ù‡Ø± Ø§Ù„Ø±Ù‚Ù… ÙÙŠ Ø§Ù„Ø³Ø­Ø¨ Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØŸ
                # âœ… Ø¥ØµÙ„Ø§Ø­: Ù†Ø¶ÙŠÙ label ÙˆØ§Ø­Ø¯ ÙÙ‚Ø· Ù„ÙƒÙ„ ØµÙ (Ù†ØªÙˆÙ‚Ø¹ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹ Ø§Ù„ÙƒÙ„ÙŠ)
                # ÙˆÙ„ÙŠØ³ 32 label Ù„ÙƒÙ„ ØµÙ (ÙƒØ§Ù† ÙŠØ³Ø¨Ø¨ Ø¹Ø¯Ù… ØªØ·Ø§Ø¨Ù‚ Ø¨ÙŠÙ† X Ùˆ y)
                for num in range(1, Config.MAX_NUMBER + 1):
                    label = 1 if num in future_draw else 0
                    labels_list.append(label)
            
            # âœ… ÙŠØ¬Ø¨ Ø£Ù† ÙŠÙƒÙˆÙ† X Ø¨Ø­Ø¬Ù… (n_samples * 32) Ùˆ y Ø¨Ø­Ø¬Ù… (n_samples * 32)
            # Ù†ÙƒØ±Ø± ÙƒÙ„ ØµÙ features 32 Ù…Ø±Ø© Ù„ÙŠØªØ·Ø§Ø¨Ù‚ Ù…Ø¹ 32 label
            if features_list:
                expanded_features = []
                for fv in features_list:
                    for _ in range(Config.MAX_NUMBER):
                        expanded_features.append(fv)
                features_array = np.array(expanded_features)
            else:
                features_array = np.array([]).reshape(0, 0)
            
            labels_array = np.array(labels_list)
            
            logger.end_operation(operation_id, 'completed', {
                'features_shape': features_array.shape,
                'labels_shape': labels_array.shape,
            })
            
            return features_array, labels_array
            
        except Exception as e:
            logger.end_operation(operation_id, 'failed', {'error': str(e)})
            raise
    
    def train(self, df: pd.DataFrame, model_name: str = 'random_forest') -> Dict:
        """ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø­Ø¯Ø¯"""
        operation_id = logger.start_operation('model_training', {
            'model': model_name,
            'data_size': len(df)
        })
        
        try:
            self.benchmark.start_monitoring(f'train_{model_name}')
            
            X, y = self.prepare_features(df)
            
            if X.shape[0] < 10:
                raise ValueError(f"Ø¨ÙŠØ§Ù†Ø§Øª ØºÙŠØ± ÙƒØ§ÙÙŠØ© Ù„Ù„ØªØ¯Ø±ÙŠØ¨: {X.shape[0]} Ø¹ÙŠÙ†Ø© ÙÙ‚Ø·")
            
            X_train, X_test, y_train, y_test = train_test_split(
                X, y, test_size=0.2, random_state=42, stratify=y
            )
            
            X_train_scaled = self.scalers[model_name].fit_transform(X_train)
            X_test_scaled = self.scalers[model_name].transform(X_test)
            
            model = self.models[model_name]
            model.fit(X_train_scaled, y_train)
            
            y_pred = model.predict(X_test_scaled)
            accuracy = accuracy_score(y_test, y_pred)
            precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)
            recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)
            
            cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=5, scoring='accuracy')
            
            if hasattr(model, 'feature_importances_'):
                self.feature_importance[model_name] = model.feature_importances_
            
            self._save_model(model_name)
            
            metrics = self.benchmark.stop_monitoring(f'train_{model_name}')
            
            result = {
                'accuracy': accuracy,
                'precision': precision,
                'recall': recall,
                'cv_scores': cv_scores.tolist(),
                'cv_mean': float(cv_scores.mean()),
                'cv_std': float(cv_scores.std()),
                'feature_importance': self.feature_importance.get(model_name, np.array([])).tolist()
            }
            
            logger.end_operation(operation_id, 'completed', {
                'accuracy': round(accuracy, 4),
                'precision': round(precision, 4),
                'recall': round(recall, 4),
                'cv_mean': round(float(cv_scores.mean()), 4),
                **metrics
            })
            
            logger.log_prediction(
                model_name=model_name,
                accuracy=accuracy,
                confidence=precision,
                features_used=[f'feature_{i}' for i in range(X.shape[1])]
            )
            
            self.is_trained = True
            
            return result
            
        except Exception as e:
            logger.end_operation(operation_id, 'failed', {'error': str(e)})
            raise
    
    def predict(self, current_numbers: List[int], df: pd.DataFrame, 
                top_n: int = 10, model_name: str = 'random_forest') -> List[Tuple[int, float]]:
        """Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„ØªØ§Ù„ÙŠØ©"""
        if not self.is_trained or model_name not in self.models:
            raise ValueError(f"Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name} ØºÙŠØ± Ù…Ø¯Ø±Ø¨")
        
        operation_id = logger.start_operation('prediction', {
            'model': model_name,
            'current_numbers': current_numbers,
            'top_n': top_n
        })
        
        try:
            self.benchmark.start_monitoring(f'predict_{model_name}')
            
            feature_vector = self._prepare_single_features(current_numbers, df)
            scaled_features = self.scalers[model_name].transform([feature_vector])
            
            predictions = []
            model = self.models[model_name]
            
            for num in range(1, Config.MAX_NUMBER + 1):
                if num in current_numbers:
                    continue
                
                try:
                    prob = model.predict_proba(scaled_features)[0][1]
                except (IndexError, AttributeError):
                    prob = 0.5
                
                predictions.append((num, float(prob)))
            
            predictions.sort(key=lambda x: x[1], reverse=True)
            top_predictions = predictions[:top_n]
            
            metrics = self.benchmark.stop_monitoring(f'predict_{model_name}')
            
            logger.end_operation(operation_id, 'completed', {
                'top_predictions': top_predictions,
                'highest_probability': top_predictions[0][1] if top_predictions else 0,
                **metrics
            })
            
            return top_predictions
            
        except Exception as e:
            logger.end_operation(operation_id, 'failed', {'error': str(e)})
            raise
    
    def _prepare_single_features(self, numbers: List[int], df: pd.DataFrame) -> np.ndarray:
        """ØªØ­Ø¶ÙŠØ± features Ù„Ø³Ø­Ø¨ ÙˆØ§Ø­Ø¯"""
        sorted_nums = sorted(numbers)
        
        basic_features = [
            *sorted_nums,
            sum(numbers),
            sum(1 for n in numbers if n % 2),
            sum(1 for i in range(len(sorted_nums)-1) if sorted_nums[i+1] - sorted_nums[i] == 1),
            sorted_nums[-1] - sorted_nums[0] if len(sorted_nums) > 1 else 0,
            float(np.mean(numbers)),
            float(np.std(numbers))
        ]
        
        # âœ… chain Ù…Ø³ØªÙˆØ±Ø¯Ø© Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­
        freq_counter = Counter(list(chain.from_iterable(df['numbers'])))
        statistical_features = [
            float(np.mean([freq_counter.get(n, 0) for n in numbers])),
            float(np.std([freq_counter.get(n, 0) for n in numbers])),
            0.0
        ]
        
        pattern_features = [
            len(set([n % 10 for n in numbers])),
            sum(1 for n in numbers if self._is_prime(n)),
            self._calculate_balance(numbers)
        ]
        
        return np.array(basic_features + statistical_features + pattern_features)
    
    def _is_prime(self, n: int) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø±Ù‚Ù… Ø£ÙˆÙ„ÙŠØ§Ù‹"""
        if n < 2:
            return False
        for i in range(2, int(np.sqrt(n)) + 1):
            if n % i == 0:
                return False
        return True
    
    def _calculate_balance(self, numbers: List[int]) -> float:
        """Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© Ø§Ù„ØªÙˆØ§Ø²Ù†"""
        if len(numbers) < 2:
            return 1.0
        
        first_half = sum(1 for n in numbers if n <= 16)
        second_half = len(numbers) - first_half
        balance = 1 - abs(first_half - second_half) / len(numbers)
        
        return balance
    
    def _save_model(self, model_name: str):
        """Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠ"""
        # âœ… Config.MODELS_DIR Ù…ÙˆØ¬ÙˆØ¯ Ø§Ù„Ø¢Ù† ÙÙŠ settings.py
        os.makedirs(Config.MODELS_DIR, exist_ok=True)
        
        model_path = os.path.join(Config.MODELS_DIR, f'{model_name}.pkl')
        scaler_path = os.path.join(Config.MODELS_DIR, f'{model_name}_scaler.pkl')
        
        joblib.dump(self.models[model_name], model_path)
        joblib.dump(self.scalers[model_name], scaler_path)
        
        logger.logger.info(f"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name}")
    
    def load_model(self, model_name: str):
        """ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­ÙÙˆØ¸"""
        model_path = os.path.join(Config.MODELS_DIR, f'{model_name}.pkl')
        scaler_path = os.path.join(Config.MODELS_DIR, f'{model_name}_scaler.pkl')
        
        if os.path.exists(model_path) and os.path.exists(scaler_path):
            self.models[model_name] = joblib.load(model_path)
            self.scalers[model_name] = joblib.load(scaler_path)
            self.is_trained = True
            logger.logger.info(f"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name}")
        else:
            raise FileNotFoundError(f"Ù…Ù„ÙØ§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name} ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯Ø©")
    
    def ensemble_predict(self, current_numbers: List[int], df: pd.DataFrame,
                        top_n: int = 10) -> List[Tuple[int, float]]:
        """ØªÙ†Ø¨Ø¤ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ensemble Ù…Ù† Ø¹Ø¯Ø© Ù†Ù…Ø§Ø°Ø¬"""
        all_predictions = []
        
        for model_name in self.models:
            try:
                predictions = self.predict(current_numbers, df, top_n=20, model_name=model_name)
                all_predictions.append(predictions)
            except Exception as e:
                logger.logger.warning(f"ÙØ´Ù„ Ø§Ù„ØªÙ†Ø¨Ø¤ Ø¨Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ {model_name}: {e}")
                continue
        
        if not all_predictions:
            return []
        
        combined_scores: Counter = Counter()
        
        for predictions in all_predictions:
            for num, prob in predictions:
                combined_scores[num] += prob
        
        for num in combined_scores:
            combined_scores[num] /= len(all_predictions)
        
        final_predictions = combined_scores.most_common(top_n)
        
        return [(num, float(score)) for num, score in final_predictions]


class RecommendationEngine:
    """Ù†Ø¸Ø§Ù… ØªÙˆØµÙŠØ§Øª Ø°ÙƒÙŠ ÙŠØ¹ØªÙ…Ø¯ Ø¹Ù„Ù‰ ØªØ¹Ù„Ù… ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
    
    def __init__(self):
        self.user_profiles: Dict[str, Dict] = {}
        self.collaborative_matrix = None
    
    def learn_preferences(self, user_id: str, selected_tickets: List[List[int]], 
                         rejected_tickets: List[List[int]] = None):
        """ØªØ¹Ù„Ù… ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…"""
        profile = {
            'selected_patterns': self._extract_patterns(selected_tickets),
            'preferred_numbers': self._get_common_numbers(selected_tickets),
            'avoided_numbers': self._get_common_numbers(rejected_tickets) if rejected_tickets else set(),
            'sum_preference': self._get_sum_preference(selected_tickets),
            'odd_even_preference': self._get_odd_even_preference(selected_tickets),
            'learning_strength': min(1.0, len(selected_tickets) / 10)
        }
        
        self.user_profiles[user_id] = profile
        
        logger.logger.info(f"ğŸ¯ ØªØ¹Ù„Ù… ØªÙØ¶ÙŠÙ„Ø§Øª Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù… {user_id}", extra={
            'selected_tickets': len(selected_tickets),
            'preferred_numbers_count': len(profile['preferred_numbers']),
            'learning_strength': profile['learning_strength']
        })
    
    def recommend(self, user_id: str, base_tickets: List[List[int]], 
                 count: int = 5) -> List[List[int]]:
        """ØªÙˆÙ„ÙŠØ¯ ØªÙˆØµÙŠØ§Øª Ù…Ø®ØµØµØ©"""
        if user_id not in self.user_profiles:
            return base_tickets[:count]
        
        profile = self.user_profiles[user_id]
        recommendations = []
        
        for base_ticket in base_tickets[:max(10, count * 2)]:
            customized = self._customize_ticket(base_ticket, profile)
            if customized and customized not in recommendations:
                recommendations.append(customized)
                if len(recommendations) >= count:
                    break
        
        # Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙÙ Ø§Ù„ØªÙˆØµÙŠØ§ØªØŒ Ø£ÙƒÙ…Ù„ Ù…Ù† Ø§Ù„ØªØ°Ø§ÙƒØ± Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©
        if len(recommendations) < count:
            for ticket in base_tickets:
                if ticket not in recommendations:
                    recommendations.append(ticket)
                    if len(recommendations) >= count:
                        break
        
        return recommendations[:count]
    
    def _extract_patterns(self, tickets: List[List[int]]) -> Dict:
        """Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø£Ù†Ù…Ø§Ø· Ù…Ù† Ø§Ù„ØªØ°Ø§ÙƒØ±"""
        if not tickets:
            return {}
        
        patterns: Dict[str, list] = {
            'consecutive_range': [],
            'shadow_range': [],
            'sum_range': [],
            'odd_range': []
        }
        
        for ticket in tickets:
            patterns['consecutive_range'].append(
                sum(1 for i in range(len(ticket)-1) if ticket[i+1] - ticket[i] == 1)
            )
            patterns['shadow_range'].append(
                sum(1 for c in Counter([n % 10 for n in ticket]).values() if c > 1)
            )
            patterns['sum_range'].append(sum(ticket))
            patterns['odd_range'].append(sum(1 for n in ticket if n % 2))
        
        result = {}
        for key in patterns:
            vals = patterns[key]
            if vals:
                result[key] = {
                    'min': min(vals),
                    'max': max(vals),
                    'avg': float(np.mean(vals))
                }
            else:
                result[key] = {'min': 0, 'max': 0, 'avg': 0.0}
        
        return result
    
    def _get_common_numbers(self, tickets: List[List[int]]) -> Set[int]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø§Ù„Ù…Ø´ØªØ±ÙƒØ©"""
        if not tickets:
            return set()
        
        counter: Counter = Counter()
        for ticket in tickets:
            counter.update(ticket)
        
        threshold = len(tickets) * 0.3
        return {num for num, count in counter.items() if count >= threshold}
    
    def _get_sum_preference(self, tickets: List[List[int]]) -> Dict:
        """ØªØ­Ø¯ÙŠØ¯ ØªÙØ¶ÙŠÙ„ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹"""
        if not tickets:
            return {'min': 20, 'max': 200, 'avg': 100.0, 'std': 0.0}
        
        sums = [sum(t) for t in tickets]
        return {
            'min': min(sums),
            'max': max(sums),
            'avg': float(np.mean(sums)),
            'std': float(np.std(sums))
        }
    
    def _get_odd_even_preference(self, tickets: List[List[int]]) -> Dict:
        """ØªØ­Ø¯ÙŠØ¯ ØªÙØ¶ÙŠÙ„ Ø§Ù„ÙØ±Ø¯ÙŠ/Ø§Ù„Ø²ÙˆØ¬ÙŠ"""
        if not tickets:
            return {'min_odd': 0, 'max_odd': 6, 'avg_odd': 3.0, 'preferred_odd': 3}
        
        odd_counts = [sum(1 for n in t if n % 2) for t in tickets]
        return {
            'min_odd': min(odd_counts),
            'max_odd': max(odd_counts),
            'avg_odd': float(np.mean(odd_counts)),
            'preferred_odd': int(np.round(np.mean(odd_counts)))
        }
    
    def _customize_ticket(self, base_ticket: List[int], profile: Dict) -> List[int]:
        """ØªØ®ØµÙŠØµ Ø§Ù„ØªØ°ÙƒØ±Ø© Ø¨Ù†Ø§Ø¡Ù‹ Ø¹Ù„Ù‰ Ø§Ù„ØªÙØ¶ÙŠÙ„Ø§Øª"""
        ticket = base_ticket.copy()
        
        preferred = profile.get('preferred_numbers', set())
        avoided = profile.get('avoided_numbers', set())
        
        for i in range(len(ticket)):
            if ticket[i] in avoided and preferred:
                for pref_num in preferred:
                    if pref_num not in ticket and Config.MIN_NUMBER <= pref_num <= Config.MAX_NUMBER:
                        ticket[i] = pref_num
                        break
        
        odd_pref = profile.get('odd_even_preference', {})
        target_odd = odd_pref.get('preferred_odd', 3)
        current_odd = sum(1 for n in ticket if n % 2)
        
        if current_odd > target_odd:
            odd_indices = [i for i, n in enumerate(ticket) if n % 2]
            changes_needed = current_odd - target_odd
            
            for idx in odd_indices[:changes_needed]:
                candidate = ticket[idx] + 1
                if candidate > Config.MAX_NUMBER:
                    candidate = ticket[idx] - 1
                if Config.MIN_NUMBER <= candidate <= Config.MAX_NUMBER and candidate not in ticket:
                    ticket[idx] = candidate
        
        elif current_odd < target_odd:
            even_indices = [i for i, n in enumerate(ticket) if n % 2 == 0]
            changes_needed = target_odd - current_odd
            
            for idx in even_indices[:changes_needed]:
                candidate = ticket[idx] + 1
                if candidate > Config.MAX_NUMBER:
                    candidate = ticket[idx] - 1
                if Config.MIN_NUMBER <= candidate <= Config.MAX_NUMBER and candidate not in ticket:
                    ticket[idx] = candidate
        
        # Ø¶Ø¨Ø· Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹
        sum_pref = profile.get('sum_preference', {})
        current_sum = sum(ticket)
        target_sum = int(sum_pref.get('avg', current_sum))
        
        if abs(current_sum - target_sum) > 10 and len(ticket) > 0:
            diff = target_sum - current_sum
            adjustment_per_num = diff // len(ticket)
            
            if abs(adjustment_per_num) > 0:
                for i in range(len(ticket)):
                    new_val = ticket[i] + adjustment_per_num
                    if Config.MIN_NUMBER <= new_val <= Config.MAX_NUMBER and new_val not in ticket:
                        ticket[i] = new_val
        
        # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ ØªÙƒØ±Ø§Ø± ÙˆØ£Ù† Ø§Ù„Ø£Ø±Ù‚Ø§Ù… Ø¶Ù…Ù† Ø§Ù„Ù†Ø·Ø§Ù‚
        ticket = sorted(list(set([
            n for n in ticket 
            if Config.MIN_NUMBER <= n <= Config.MAX_NUMBER
        ])))
        
        return ticket
